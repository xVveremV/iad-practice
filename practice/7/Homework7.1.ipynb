{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "354a08c2-c407-460d-abcb-a76960c25d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package genesis to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package nps_chat to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1 (Moby Dick) перші 10 слів: ['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.']\n",
      "Text5 (Chat corpus) перші 10 слів: ['now', 'im', 'left', 'with', 'this', 'gay', 'name', ':P', 'PART', 'hey']\n",
      "\n",
      "Токени: ['Data', 'science', 'is', 'a', 'multi-disciplinary', 'field', 'that', 'uses', 'scientific', 'methods', ',', 'processes', ',', 'algorithms', 'and', 'systems', 'to', 'extract', 'knowledge', 'and', 'insights', 'from', 'structured', 'and', 'unstructured', 'data', '.']\n",
      "\n",
      "Токени без стоп-слів: ['Data', 'science', 'multi-disciplinary', 'field', 'uses', 'scientific', 'methods', ',', 'processes', ',', 'algorithms', 'systems', 'extract', 'knowledge', 'insights', 'structured', 'unstructured', 'data', '.']\n",
      "\n",
      "Стеми: ['data', 'scienc', 'multi-disciplinari', 'field', 'use', 'scientif', 'method', ',', 'process', ',', 'algorithm', 'system', 'extract', 'knowledg', 'insight', 'structur', 'unstructur', 'data', '.']\n",
      "\n",
      "Леми: ['Data', 'science', 'multi-disciplinary', 'field', 'us', 'scientific', 'method', ',', 'process', ',', 'algorithm', 'system', 'extract', 'knowledge', 'insight', 'structured', 'unstructured', 'data', '.']\n",
      "\n",
      "POS-теги: [('Data', 'NNP'), ('science', 'NN'), ('multi-disciplinary', 'JJ'), ('field', 'NN'), ('uses', 'VBZ'), ('scientific', 'JJ'), ('methods', 'NNS'), (',', ','), ('processes', 'NNS'), (',', ','), ('algorithms', 'JJ'), ('systems', 'NNS'), ('extract', 'JJ'), ('knowledge', 'JJ'), ('insights', 'NNS'), ('structured', 'VBN'), ('unstructured', 'JJ'), ('data', 'NNS'), ('.', '.')]\n",
      "\n",
      "Довгі слова в text1: ['CIRCUMNAVIGATION', 'perpendicularly', 'unceremoniously', 'uncomfortableness', 'sympathetically', 'Ehrenbreitstein', 'multitudinously', 'phrenologically', 'cannibalistically', 'comfortableness', 'intolerableness', 'perpendicularly', 'unconsciousness', 'circumnavigations', 'untraditionally', 'heterogeneously', 'superfluousness', 'superstitiousness', 'apprehensiveness', 'superstitiously']\n",
      "\n",
      "Часті слова в chat corpus: ['.', 'JOIN', 'PART', '?', 'lol', 'to', 'i', 'the', 'you', ',', 'I', 'a', 'hi', 'me', '...', 'is', '..', 'in', 'ACTION', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, gutenberg, genesis, nps_chat\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, FreqDist\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Завантаження ресурсів\n",
    "# ------------------------------\n",
    "nltk.download('punkt')                        # Завантажуємо модель токенізації речень та слів\n",
    "nltk.download('stopwords')                    # Завантажуємо список стоп-слів англійської мови\n",
    "nltk.download('wordnet')                      # Завантажуємо WordNet для лематизації\n",
    "nltk.download('omw-1.4')                      # Завантажуємо багатомовну підтримку для лематизації\n",
    "nltk.download('averaged_perceptron_tagger')   # Завантажуємо POS-тегінг (частини мови)\n",
    "nltk.download('gutenberg')                    # Завантажуємо корпус текстів Gutenberg\n",
    "nltk.download('genesis')                      # Завантажуємо Genesis corpus\n",
    "nltk.download('nps_chat')                     # Завантажуємо чат-корпус NPS Chat\n",
    "\n",
    "# Для NLTK 3.9.1 + POS-тегінг:\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # Завантажуємо англійський POS-тегер (версія для NLTK 3.9.1)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Перегляд текстів із корпусів\n",
    "# ------------------------------\n",
    "text1_words = gutenberg.words('melville-moby_dick.txt')  # Завантажуємо слова з \"Moby Dick\"\n",
    "text5_words = nps_chat.words()                           # Завантажуємо слова з чат-корпусу\n",
    "\n",
    "print(\"Text1 (Moby Dick) перші 10 слів:\", text1_words[:10])  # Виводимо перші 10 слів тексту 1\n",
    "print(\"Text5 (Chat corpus) перші 10 слів:\", text5_words[:10]) # Виводимо перші 10 слів тексту 5\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Токенізація та стоп-слова\n",
    "# ------------------------------\n",
    "sample_text = (\"Data science is a multi-disciplinary field that uses scientific \"\n",
    "               \"methods, processes, algorithms and systems to extract knowledge and \"\n",
    "               \"insights from structured and unstructured data.\")  # Приклад тексту для аналізу\n",
    "\n",
    "tokens = word_tokenize(sample_text)              # Розбиваємо текст на слова (токенізація)\n",
    "stop_words = set(stopwords.words('english'))     # Отримуємо множину стоп-слів\n",
    "filtered_tokens = [w for w in tokens if w.lower() not in stop_words]  # Видаляємо стоп-слова\n",
    "\n",
    "print(\"\\nТокени:\", tokens)                       # Виводимо всі токени\n",
    "print(\"\\nТокени без стоп-слів:\", filtered_tokens) # Виводимо токени без стоп-слів\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Стемінг та лемматизація\n",
    "# ------------------------------\n",
    "stemmer = PorterStemmer()                        # Ініціалізація стемера (PorterStemmer)\n",
    "lemmatizer = WordNetLemmatizer()                # Ініціалізація лематизатора (WordNet)\n",
    "\n",
    "stems = [stemmer.stem(w) for w in filtered_tokens]       # Стемінг кожного слова\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in filtered_tokens] # Лематизація кожного слова\n",
    "\n",
    "print(\"\\nСтеми:\", stems)                         # Виводимо результати стемінгу\n",
    "print(\"\\nЛеми:\", lemmas)                         # Виводимо результати лематизації\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Частини мови (POS tagging)\n",
    "# ------------------------------\n",
    "pos_tags = pos_tag(filtered_tokens)             # Присвоюємо кожному слову його частину мови\n",
    "print(\"\\nPOS-теги:\", pos_tags)                  # Виводимо список слів із POS-тегами\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Пошук довгих і частих слів у корпусах\n",
    "# ------------------------------\n",
    "# Довгі слова (>=15 букв) у Moby Dick\n",
    "long_words_text1 = [w for w in text1_words if len(w) >= 15]  # Відбираємо слова довші або рівні 15 букв\n",
    "\n",
    "# Часті слова у Chat corpus\n",
    "freq_chat = FreqDist(text5_words)              # Обчислюємо частоти слів у чат-корпусі\n",
    "most_common_chat = [w for w, _ in freq_chat.most_common(20)]  # Беремо 20 найчастіших слів\n",
    "\n",
    "print(\"\\nДовгі слова в text1:\", long_words_text1[:20])       # Виводимо перші 20 довгих слів\n",
    "print(\"\\nЧасті слова в chat corpus:\", most_common_chat)       # Виводимо 20 найчастіших слів\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
